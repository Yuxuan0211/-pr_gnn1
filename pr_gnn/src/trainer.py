# src/trainer.py
import torch
from torch.optim import Adam
try:
    from torch.optim.lr_scheduler import ReduceLROnPlateau
except ImportError:
    print("âš ï¸  æ— æ³•å¯¼å…¥ReduceLROnPlateauï¼Œå°†ä½¿ç”¨ç®€åŒ–ç‰ˆå®ç°")
    class ReduceLROnPlateau:
        def __init__(self, optimizer, mode='min', factor=0.1, patience=10, 
                    min_lr=0, verbose=False):
            self.optimizer = optimizer
            self.mode = mode
            self.factor = factor
            self.patience = patience
            self.min_lr = min_lr
            self.verbose = verbose
            self.best = float('inf') if mode == 'min' else -float('inf')
            self.num_bad_epochs = 0
            
        def step(self, metrics):
            if self.mode == 'min':
                is_better = metrics < self.best
            else:
                is_better = metrics > self.best
                
            if is_better:
                self.best = metrics
                self.num_bad_epochs = 0
            else:
                self.num_bad_epochs += 1
                
            if self.num_bad_epochs >= self.patience:
                self._reduce_lr()
                self.num_bad_epochs = 0
                
        def _reduce_lr(self):
            for param_group in self.optimizer.param_groups:
                old_lr = param_group['lr']
                new_lr = max(old_lr * self.factor, self.min_lr)
                if old_lr - new_lr > 1e-8:
                    param_group['lr'] = new_lr
                    if self.verbose:
                        print(f'å°†å­¦ä¹ ç‡ä» {old_lr:.2e} é™ä½åˆ° {new_lr:.2e}')

import math
from tqdm import tqdm
import os
import sys
import numpy as np
from typing import Dict, List, Optional, Tuple
from torch_geometric.loader import NeighborSampler
from torch_sparse import SparseTensor

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(project_root)

from pr_gnn.src.physics_loss import PhysicsLoss
from pr_gnn.src.assign_regions import get_regional_masks

class SimpleData:
    def __init__(self, y, pos=None):
        # ç¡®ä¿yæ˜¯å¼ é‡
        if isinstance(y, torch.Tensor):
            self.y = y
        elif isinstance(y, (tuple, list)):
            print(f"âš ï¸  Warning: Converting tuple/list to tensor (length: {len(y)})")
            try:
                self.y = torch.stack(y) if len(y) > 0 else torch.tensor([])
            except Exception as e:
                raise ValueError(f"æ— æ³•å°†tuple/listè½¬æ¢ä¸ºå¼ é‡: {str(e)}")
        else:
            try:
                self.y = torch.as_tensor(y)
                if not isinstance(self.y, torch.Tensor):
                    raise ValueError(f"è½¬æ¢å¤±è´¥ï¼Œç»“æœç±»å‹: {type(self.y)}")
            except Exception as e:
                raise ValueError(f"æ— æ³•å°†è¾“å…¥è½¬æ¢ä¸ºå¼ é‡ï¼Œç±»å‹: {type(y)}. é”™è¯¯: {str(e)}")
        
        # ç¡®ä¿yæ˜¯äºŒç»´å¼ é‡ (num_nodes, num_features)
        if len(self.y.shape) == 1:
            self.y = self.y.unsqueeze(1)
        elif len(self.y.shape) != 2:
            raise ValueError(f"yçš„å½¢çŠ¶åº”ä¸º2D (num_nodes, num_features)ï¼Œå®é™…ä¸º: {self.y.shape}")
            
        self.pos = pos if pos is not None else None
    
    def __getattr__(self, name):
        # å°†å±æ€§è®¿é—®è½¬å‘åˆ°yå¼ é‡
        if name in ['size', 'shape', 'device', 'dtype']:
            return getattr(self.y, name)
        raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
    
    def __iter__(self):
        # é˜²æ­¢è¢«å½“ä½œå…ƒç»„å¤„ç†
        raise TypeError(f"'{type(self).__name__}' object is not iterable")
    
    def __array__(self):
        # æ”¯æŒnumpyè½¬æ¢
        return self.y.numpy()
    
    def __torch_function__(self, func, types, args=(), kwargs=None):
        # æ”¯æŒtorchå‡½æ•°è°ƒç”¨
        if kwargs is None:
            kwargs = {}
        return func(self.y, *args, **kwargs)

class PRGNNTrainer:
    def __init__(self, model, config):
        self.model = model
        self.config = config
        self.device = config.get('device', 'cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        
        # æ··åˆç²¾åº¦è®­ç»ƒåˆå§‹åŒ–
        self.scaler = torch.cuda.amp.GradScaler(enabled=config['training']['mixed_precision'])
        
        # ä¼˜åŒ–å™¨åˆå§‹åŒ– (AdamW with weight decay)
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config['training']['lr'],
            weight_decay=config['training']['weight_decay']
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨ (é¢„çƒ­ + ä½™å¼¦è¡°å‡)
        warmup_epochs = config['training']['warmup_epochs']
        cosine_epochs = config['training']['cosine_epochs']
        total_steps = warmup_epochs + cosine_epochs
        
        def lr_lambda(current_step):
            if current_step < warmup_epochs:
                return float(current_step) / float(max(1, warmup_epochs))
            progress = float(current_step - warmup_epochs) / float(max(1, cosine_epochs))
            return 0.5 * (1.0 + math.cos(math.pi * progress))
            
        self.scheduler = torch.optim.lr_scheduler.LambdaLR(
            self.optimizer, 
            lr_lambda,
            last_epoch=-1
        )
        
        # ç‰©ç†æŸå¤±è®¡ç®—å™¨
        self.physics_loss = PhysicsLoss(config)
        
        # è®­ç»ƒçŠ¶æ€è®°å½•
        self.train_state = {
            'train_loss_history': [],
            'val_loss_history': [],
            'best_val_loss': float('inf'),
            'best_epoch': 0,
            'converge_count': 0,
            'current_epoch': 0,
            'lr_history': []
        }
        
        # åŒºåŸŸæ©ç ç¼“å­˜
        self.region_mask_cache = None

    def _get_region_mask(self, data) -> torch.Tensor:
        """è·å–åŒºåŸŸæ©ç ï¼ˆå¸¦ç¼“å­˜æœºåˆ¶ï¼‰"""
        if self.region_mask_cache is None or len(self.region_mask_cache) != len(data.y):
            masks = get_regional_masks(data.y)
            region_mask = torch.zeros(len(data.y), dtype=torch.long, device=self.device)
            region_mask[masks["shock"]] = 0
            region_mask[masks["boundary"]] = 1
            region_mask[masks["wake"]] = 2
            region_mask[masks["inviscid"]] = 3
            region_mask[masks["freestream"]] = 4
            self.region_mask_cache = region_mask
        return self.region_mask_cache

    def regional_pretrain(self, data, val_data=None):
        data = data.to(self.device)
        if val_data is not None:
            val_data = val_data.to(self.device)
        
        self.model.train()
        region_mask = self._get_region_mask(data)

        # æ ¹æ®èŠ‚ç‚¹æ•°åŠ¨æ€è°ƒæ•´è®­ç»ƒè½®æ•°
        total_nodes = len(data.y)
        base_epochs = self.config.get('pre_epochs', 100)
        scale_factor = min(1.0, 10000 / total_nodes)
        adjusted_epochs = max(10, int(base_epochs * scale_factor))
        print(f"ğŸ“Š åŒºåŸŸé¢„è®­ç»ƒé…ç½®ï¼šæ€»èŠ‚ç‚¹æ•°{total_nodes}ï¼Œè°ƒæ•´åè½®æ•°{adjusted_epochs}")

        # æŒ‰åŒºåŸŸè¿›è¡Œé¢„è®­ç»ƒ
        for region_id in range(5):
            region_mask_bool = (region_mask == region_id)
            region_nodes = region_mask_bool.sum().item()
            
            if region_nodes == 0:
                print(f"â© åŒºåŸŸ {region_id} æ— èŠ‚ç‚¹ï¼Œè‡ªåŠ¨è¿›å…¥ä¸‹ä¸€åŒºåŸŸ")
                continue
                
            print(f"\n=== å¼€å§‹é¢„è®­ç»ƒåŒºåŸŸ {region_id} ===")
            print(f"ğŸ“Š åŒºåŸŸèŠ‚ç‚¹æ•°: {region_nodes}")
            print(f"â³ è®­ç»ƒè½®æ•°: {adjusted_epochs}")
            
            # é‡ç½®è¯¥åŒºåŸŸçš„æ”¶æ•›è®¡æ•°å™¨
            self.train_state['converge_count'] = 0
            
            for epoch in range(adjusted_epochs):
                self.train_state['current_epoch'] += 1
                current_lr = self.optimizer.param_groups[0]['lr']
                self.train_state['lr_history'].append(current_lr)

                # è®­ç»ƒæ­¥éª¤
                self.optimizer.zero_grad()
                pred, _ = self.model(data.x, data.edge_index)
                # å°†å¸ƒå°”æ©ç è½¬æ¢ä¸ºæ•´æ•°ç´¢å¼•
                region_indices = torch.where(region_mask_bool)[0]
                
                # å¤„ç†posæ•°æ®
                pos = None
                if hasattr(data, 'pos') and data.pos is not None:
                    pos = data.pos[region_indices]
                
                # ç›´æ¥ä¼ é€’yå¼ é‡è€Œä¸æ˜¯å°è£…å¯¹è±¡
                temp_data = SimpleData(
                    y=data.y[region_indices],
                    pos=pos
                )
                
                total_loss, loss_dict = self.physics_loss(
                    pred[region_indices],
                    temp_data,
                    region_mask[region_indices]
                )
                total_loss.backward()
                self.optimizer.step()
                
                # æ¯10è½®æ‰“å°ä¸€æ¬¡è¿›åº¦
                if (epoch + 1) % 10 == 0 or epoch == adjusted_epochs - 1:
                    print(f"ğŸ åŒºåŸŸ {region_id} - è½®æ¬¡ {epoch + 1}/{adjusted_epochs} - å½“å‰æŸå¤±: {total_loss.item():.4f}")
                
                # è®°å½•è®­ç»ƒæŸå¤±
                self.train_state['train_loss_history'].append(total_loss.item())

                # éªŒè¯é›†è¯„ä¼°ï¼ˆå¦‚æœæœ‰ï¼‰
                val_loss_dict = {}
                if val_data is not None:
                    val_loss_dict = self._evaluate(val_data)
                    self.train_state['val_loss_history'].append(val_loss_dict['val_total_loss'])
                    # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
                    self.scheduler.step(val_loss_dict['val_total_loss'])

                # æ‰“å°æ—¥å¿—ï¼ˆæ¯5ä¸ªepochï¼‰
                if epoch % 5 == 0:
                    log_msg = (f"åŒºåŸŸ {region_id} | Epoch {epoch:3d}/{adjusted_epochs} | LR: {current_lr:.6f} | "
                               f"Train Loss: {total_loss.item():.6f} | "
                               f"Sup Loss: {loss_dict['L_supervised']:.6f}")
                    if val_data is not None:
                        log_msg += f" | Val Loss: {val_loss_dict['val_total_loss']:.6f}"
                    print(log_msg)

                # æ”¶æ•›æ£€éªŒï¼ˆå¦‚æœæœ‰éªŒè¯é›†ï¼‰
                if val_data is not None:
                    is_converged, converge_msg = self._check_convergence()
                    print(f"ğŸ” æ”¶æ•›çŠ¶æ€: {converge_msg}")
                    
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    if self.train_state['current_epoch'] == self.train_state['best_epoch']:
                        self.save_model(f"models/best_pretrain_region_{region_id}.pth")
                    
                    # å¦‚æœæ”¶æ•›ï¼Œæå‰åœæ­¢è¯¥åŒºåŸŸè®­ç»ƒ
                    if is_converged:
                        print(f"ğŸ‰ åŒºåŸŸ {region_id} é¢„è®­ç»ƒæå‰æ”¶æ•›ï¼Œåœæ­¢è®­ç»ƒ")
                        break

            print(f"âœ… åŒºåŸŸ {region_id} é¢„è®­ç»ƒå®Œæˆ")

        # è¿”å›è®­ç»ƒå†å²
        return {
            'train_loss': self.train_state['train_loss_history'],
            'val_loss': self.train_state['val_loss_history'],
            'lr_history': self.train_state['lr_history']
        }

    def global_finetune(self, data, epochs, val_data=None):
        data = data.to(self.device)
        if val_data is not None:
            val_data = val_data.to(self.device)
        
        self.model.train()
        region_mask = self._get_region_mask(data)
        total_nodes = len(data.y)
        
        # åˆå§‹åŒ–train_loader
        train_loader = None
        
        # é‚»å±…é‡‡æ ·é…ç½®
        if self.config['training']['neighbor_sampling']:
            num_neighbors = [self.config['training']['num_neighbors']] * self.config['training']['num_layers']
            batch_size = min(2048, total_nodes)  # é‚»å±…é‡‡æ ·æ—¶ä½¿ç”¨è¾ƒå°çš„batch size
            
            train_loader = NeighborSampler(
                data.edge_index, 
                node_idx=None, 
                sizes=num_neighbors,
                batch_size=batch_size,
                shuffle=True,
                num_workers=0
            )
            print(f"ğŸ“Š å…¨å±€å¾®è°ƒé…ç½®ï¼šæ€»èŠ‚ç‚¹æ•°{total_nodes}ï¼Œé‚»å±…é‡‡æ ·batch size={batch_size}ï¼Œé‚»å±…æ•°={num_neighbors}")
        else:
            # åŠ¨æ€è°ƒæ•´batch size
            min_batch_size = 64
            max_batch_size = 2048
            target_batches = 100  # ç›®æ ‡æ‰¹æ¬¡æ•°
            
            # æ™ºèƒ½è®¡ç®—batch size
            batch_size = min(
                max_batch_size,
                max(min_batch_size, total_nodes // target_batches)
            )
            
            # åˆ›å»ºè™šæ‹Ÿtrain_loaderä»¥ä¿æŒä»£ç ç»“æ„ä¸€è‡´
            train_loader = [(batch_size, torch.arange(total_nodes), [])]  # ä½¿ç”¨å…ƒç»„æ¨¡æ‹ŸNeighborSamplerè¾“å‡º
            
            # ç¡®ä¿ä¸è¶…è¿‡æ˜¾å­˜é™åˆ¶
            if torch.cuda.is_available():
                free_mem = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)
                safe_batch_size = min(batch_size, free_mem // (1024 * 1024 * 10))  # ä¼°ç®—10MB/æ ·æœ¬
                if safe_batch_size < batch_size:
                    print(f"âš ï¸  æ˜¾å­˜é™åˆ¶ï¼Œbatch sizeä»{batch_size}è°ƒæ•´ä¸º{safe_batch_size}")
                    batch_size = safe_batch_size
            
            print(f"ğŸ“Š å…¨å±€å¾®è°ƒé…ç½®ï¼šæ€»èŠ‚ç‚¹æ•°{total_nodes}ï¼Œbatch size={batch_size}ï¼Œæœ€å¤§è½®æ•°={epochs}")

        # æ¢¯åº¦ç´¯ç§¯å‚æ•°
        grad_accum_steps = self.config.get('grad_accum_steps', 1)
        if grad_accum_steps > 1:
            print(f"âš ï¸  å¯ç”¨æ¢¯åº¦ç´¯ç§¯ï¼Œç´¯ç§¯æ­¥æ•°ï¼š{grad_accum_steps}")

        # å¾®è°ƒè®­ç»ƒå¾ªç¯
        with tqdm(range(epochs), desc="å…¨å±€å¾®è°ƒ", unit="epoch") as pbar:
            for epoch in pbar:
                # æ›´æ–°è¿›åº¦æ¡æè¿°
                pbar.set_postfix({
                    'batch': f"{batch_size}/{total_nodes}",
                    'lr': f"{self.optimizer.param_groups[0]['lr']:.1e}"
                })
                
                if self.config['training']['neighbor_sampling']:
                    # é‚»å±…é‡‡æ ·è®­ç»ƒ
                    total_train_loss = 0.0
                batch_count = 0
                
                for batch_size, n_id, adjs in train_loader:
                    adjs = [adj.to(self.device) for adj in adjs]
                    self.optimizer.zero_grad() if batch_count % grad_accum_steps == 0 else None
                    
                    # æ··åˆç²¾åº¦è®­ç»ƒ
                    with torch.cuda.amp.autocast(enabled=self.config['training']['mixed_precision']):
                        # å°†adjsè½¬æ¢ä¸ºSparseTensoræ ¼å¼
                        if len(adjs) > 0 and hasattr(adjs[0], 'edge_index'):
                            # ç¡®ä¿åªä½¿ç”¨å½“å‰æ‰¹æ¬¡çš„èŠ‚ç‚¹
                            batch_nodes = n_id[:batch_size]
                            adj = SparseTensor(
                                row=adjs[0].edge_index[0],
                                col=adjs[0].edge_index[1],
                                sparse_sizes=(len(batch_nodes), len(batch_nodes))
                            )
                            out = self.model(data.x[batch_nodes], adj)
                        else:
                            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„é‚»æ¥ä¿¡æ¯ï¼Œä½¿ç”¨å…¨è¿æ¥
                            batch_nodes = n_id[:batch_size]
                            adj = SparseTensor(
                                row=torch.arange(len(batch_nodes), device=self.device),
                                col=torch.arange(len(batch_nodes), device=self.device),
                                sparse_sizes=(len(batch_nodes), len(batch_nodes))
                            )
                            out = self.model(data.x[batch_nodes], adj)
                        
                        # å¤„ç†posæ•°æ®
                        pos = None
                        if hasattr(data, 'pos') and data.pos is not None:
                            pos = data.pos[n_id]
                        
                # è°ƒè¯•æ—¥å¿—
                print(f"data.y type before SimpleData: {type(data.y)}")
                if hasattr(data.y, 'shape'):
                    print(f"data.y shape: {data.y.shape}")
                elif isinstance(data.y, (tuple, list)):
                    print(f"data.y length: {len(data.y)}")
                
                # ç¡®ä¿yæ˜¯tensor
                y_tensor = torch.as_tensor(data.y[n_id]) if not isinstance(data.y[n_id], torch.Tensor) else data.y[n_id]
                print(f"y_tensor type: {type(y_tensor)}, shape: {y_tensor.shape}")
                
                temp_data = SimpleData(
                    y=y_tensor,
                    pos=pos
                )
                
                batch_loss, _ = self.physics_loss(
                    out,
                    temp_data,
                    region_mask[n_id]
                )
                
                batch_loss = batch_loss / grad_accum_steps
                self.scaler.scale(batch_loss).backward()
                total_train_loss += batch_loss.item() * grad_accum_steps
                batch_count += 1

                # ç´¯ç§¯åˆ°æŒ‡å®šæ­¥æ•°åæ›´æ–°å‚æ•°
                if batch_count % grad_accum_steps == 0:
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                    self.scheduler.step()
                
                # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±
                avg_train_loss = total_train_loss / batch_count
                self.train_state['train_loss_history'].append(avg_train_loss)
            else:
                self.train_state['current_epoch'] += 1
            current_lr = self.optimizer.param_groups[0]['lr']
            self.train_state['lr_history'].append(current_lr)
            
            total_train_loss = 0.0
            batch_count = 0

            # åˆ†æ‰¹å¤„ç†å¤§æ•°æ®é›†
            for i in range(0, total_nodes, batch_size):
                # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„æ©ç 
                batch_end = min(i + batch_size, total_nodes)
                batch_mask = slice(i, batch_end)
                
                # æ‰¹æ¬¡è®­ç»ƒï¼ˆæ¢¯åº¦ç´¯ç§¯ï¼‰
                self.optimizer.zero_grad() if batch_count % grad_accum_steps == 0 else None
                
                pred, _ = self.model(data.x, data.edge_index)
                # åªè®¡ç®—å½“å‰æ‰¹æ¬¡çš„æŸå¤±
                # å¤„ç†posæ•°æ®
                pos = None
                if hasattr(data, 'pos') and data.pos is not None:
                    pos = data.pos[batch_mask]
                
                temp_data = SimpleData(
                    y=data.y[batch_mask],
                    pos=pos
                )
                
                batch_loss, _ = self.physics_loss(
                    pred[batch_mask],
                    temp_data,
                    region_mask[batch_mask]
                )
                
                # æ¢¯åº¦ç´¯ç§¯ï¼šæŸå¤±é™¤ä»¥ç´¯ç§¯æ­¥æ•°
                batch_loss = batch_loss / grad_accum_steps
                batch_loss.backward()
                
                total_train_loss += batch_loss.item() * grad_accum_steps
                batch_count += 1

                # ç´¯ç§¯åˆ°æŒ‡å®šæ­¥æ•°åæ›´æ–°å‚æ•°
                if batch_count % grad_accum_steps == 0 or batch_count == (total_nodes // batch_size + 1):
                    self.optimizer.step()

            # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±
            avg_train_loss = total_train_loss / batch_count
            self.train_state['train_loss_history'].append(avg_train_loss)

            # éªŒè¯é›†è¯„ä¼°ï¼ˆå¦‚æœæœ‰ï¼‰
            val_loss_dict = {}
            if val_data is not None:
                val_loss_dict = self._evaluate(val_data)
                self.train_state['val_loss_history'].append(val_loss_dict['val_total_loss'])
                # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
                self.scheduler.step(val_loss_dict['val_total_loss'])

            # æ‰“å°æ—¥å¿—ï¼ˆæ¯10ä¸ªepochï¼‰
            if epoch % 10 == 0:
                log_msg = (f"å¾®è°ƒ Epoch {epoch:3d}/{epochs} | LR: {current_lr:.6f} | "
                           f"Avg Train Loss: {avg_train_loss:.6f}")
                if val_data is not None:
                    log_msg += (f" | Val Total Loss: {val_loss_dict['val_total_loss']:.6f} | "
                               f"Val Sup Loss: {val_loss_dict['L_supervised']:.6f} | "
                               f"Val Phys Loss: {val_loss_dict['L_thermo'] + val_loss_dict['L_vorticity']:.6f}")
                print(log_msg)

            # æ”¶æ•›æ£€éªŒï¼ˆå¦‚æœæœ‰éªŒè¯é›†ï¼‰
            if val_data is not None:
                is_converged, converge_msg = self._check_convergence()
                print(f"ğŸ” æ”¶æ•›çŠ¶æ€: {converge_msg}")
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if self.train_state['current_epoch'] == self.train_state['best_epoch']:
                    self.save_model("models/best_finetune.pth")
                    self.save_train_state("models/best_finetune_state.pth")
                
            # å¦‚æœæ”¶æ•›ï¼Œæå‰åœæ­¢å¾®è°ƒ
            if is_converged:
                print(f"ğŸ‰ å…¨å±€å¾®è°ƒæå‰æ”¶æ•›ï¼Œåœæ­¢è®­ç»ƒï¼ˆæ€»è®­ç»ƒè½®æ•°ï¼š{self.train_state['current_epoch']}ï¼‰")
                return {
                    'train_loss': self.train_state['train_loss_history'],
                    'val_loss': self.train_state['val_loss_history'],
                    'lr_history': self.train_state['lr_history']
                }

        print("=== å…¨å±€å¾®è°ƒå®Œæˆ ===")

        # è¿”å›è®­ç»ƒå†å²
        return {
            'train_loss': self.train_state['train_loss_history'],
            'val_loss': self.train_state['val_loss_history'],
            'lr_history': self.train_state['lr_history']
        }

    def _check_convergence(self) -> Tuple[bool, str]:
        """
        æ”¶æ•›æ£€éªŒé€»è¾‘
        è¿”å›ï¼š(æ˜¯å¦æ”¶æ•›, æ”¶æ•›çŠ¶æ€ä¿¡æ¯)
        """
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å°è®­ç»ƒè½®æ•°
        if self.train_state['current_epoch'] < self.config.get('min_epochs', 50):
            return False, f"æœªè¾¾åˆ°æœ€å°è®­ç»ƒè½®æ•°ï¼ˆå½“å‰{self.train_state['current_epoch']}/{self.config.get('min_epochs', 50)}ï¼‰"
        
        # è·å–ç›‘æ§æŒ‡æ ‡çš„å†å²è®°å½•
        if not self.train_state['val_loss_history']:
            return False, "æ— éªŒè¯æŸå¤±è®°å½•ï¼Œæ— æ³•åˆ¤æ–­æ”¶æ•›"
        
        current_metric = self.train_state['val_loss_history'][-1]
        best_metric = self.train_state['best_val_loss']
        
        # åˆ¤æ–­æ˜¯å¦æœ‰æ”¹è¿›
        improvement = best_metric - current_metric
        min_delta = self.config.get('converge_min_delta', 1e-6)
        
        if improvement > min_delta:
            # æœ‰æ˜¾è‘—æ”¹è¿›ï¼šæ›´æ–°æœ€ä½³çŠ¶æ€
            self.train_state['best_val_loss'] = current_metric
            self.train_state['best_epoch'] = self.train_state['current_epoch']
            self.train_state['converge_count'] = 0
            return False, f"æŒ‡æ ‡æ”¹è¿›{improvement:.6f}ï¼Œæ›´æ–°æœ€ä½³çŠ¶æ€ï¼ˆepoch {self.train_state['best_epoch']}ï¼‰"
        else:
            # æ— æ˜¾è‘—æ”¹è¿›ï¼šç´¯è®¡è®¡æ•°
            self.train_state['converge_count'] += 1
            if self.train_state['converge_count'] >= self.config.get('converge_patience', 15):
                # è¾¾åˆ°å®¹å¿ä¸Šé™ï¼Œåˆ¤å®šæ”¶æ•›
                return True, (f"è¿ç»­{self.train_state['converge_count']}ä¸ªepochæ— æ˜¾è‘—æ”¹è¿›ï¼ˆé˜ˆå€¼{min_delta}ï¼‰ï¼Œ"
                              f"è®­ç»ƒæ”¶æ•›ï¼ˆæœ€ä½³epoch: {self.train_state['best_epoch']}, æœ€ä½³éªŒè¯æŸå¤±: {best_metric:.6f}ï¼‰")
            else:
                return False, f"æ— æ˜¾è‘—æ”¹è¿›ï¼Œç´¯è®¡è®¡æ•°{self.train_state['converge_count']}/{self.config.get('converge_patience', 15)}"

    def _evaluate(self, data) -> Dict[str, float]:
        """
        éªŒè¯é›†è¯„ä¼°ï¼ˆæ— æ¢¯åº¦è®¡ç®—ï¼‰
        è¿”å›ï¼šå„æŸå¤±æŒ‡æ ‡å­—å…¸
        """
        self.model.eval()
        data = data.to(self.device)
        region_mask = self._get_region_mask(data)
        
        with torch.no_grad():
            pred, _ = self.model(data.x, data.edge_index)
            
            # åˆ›å»ºä¸´æ—¶æ•°æ®å¯¹è±¡ä»¥åŒ¹é…physics_lossæ¥å£
            temp_data = SimpleData(
                y=data.y,
                pos=data.pos if hasattr(data, 'pos') else None
            )
            
            total_loss, loss_dict = self.physics_loss(
                pred,
                temp_data,
                region_mask
            )
        
        # è¡¥å……æ€»æŸå¤±åˆ°è¿”å›å­—å…¸
        loss_dict['val_total_loss'] = total_loss.item()
        self.model.train()  # æ¢å¤è®­ç»ƒæ¨¡å¼
        return loss_dict

    def save_model(self, path):
        torch.save(self.model.state_dict(), path)
        print(f"æ¨¡å‹å·²ä¿å­˜è‡³: {path}")

    def save_train_state(self, path: str) -> None:
        """ä¿å­˜è®­ç»ƒçŠ¶æ€ï¼ˆç”¨äºä¸­æ–­åæ¢å¤ï¼‰"""
        save_dict = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'train_state': self.train_state,
            'config': self.config
        }
        # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(path), exist_ok=True)
        torch.save(save_dict, path)
        print(f"âœ… è®­ç»ƒçŠ¶æ€å·²ä¿å­˜è‡³: {path}")

    def load_train_state(self, path: str) -> None:
        """åŠ è½½è®­ç»ƒçŠ¶æ€ï¼ˆä»ä¸­æ–­å¤„æ¢å¤ï¼‰"""
        checkpoint = torch.load(path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        self.train_state = checkpoint['train_state']
        self.config = checkpoint['config']
        print(f"âœ… è®­ç»ƒçŠ¶æ€å·²åŠ è½½ï¼ˆå½“å‰epoch: {self.train_state['current_epoch']}, æœ€ä½³éªŒè¯æŸå¤±: {self.train_state['best_val_loss']:.6f}ï¼‰")

    def load_model(self, path):
        self.model.load_state_dict(torch.load(path, map_location=self.device))
        print(f"æ¨¡å‹å·²åŠ è½½: {path}")

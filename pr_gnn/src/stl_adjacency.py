import numpy as np
import trimesh
import sys
import time
import os
from joblib import Parallel, delayed

def verify_same_dir_file(file_name):
    """éªŒè¯æ–‡ä»¶æ˜¯å¦åœ¨è„šæœ¬å½“å‰ç›®å½•ï¼Œè¿”å›å®Œæ•´ç›¸å¯¹è·¯å¾„"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    file_path = os.path.join(script_dir, file_name)
    if not os.path.exists(file_path):
        raise FileNotFoundError(
            f"âŒ æ–‡ä»¶ {file_name} æœªæ‰¾åˆ°ï¼\n"
            f"   è¯·å°† {file_name} æ”¾åœ¨è„šæœ¬æ‰€åœ¨ç›®å½•ï¼š{script_dir}"
        )
    return file_path

def fast_read_stl(stl_file_name="export.stl"):
    """è¯»å–åŒç›®å½•ä¸‹çš„export.stlï¼Œæå–ä¸‰è§’å½¢ç´¢å¼•å’Œé¡¶ç‚¹æ•°"""
    start_time = time.time()
    try:
        stl_path = verify_same_dir_file(stl_file_name)
        mesh = trimesh.load(
            stl_path,
            force='mesh',
            skip_materials=True,
            skip_textures=True,
            validate=False
        )
        faces = mesh.faces.astype(np.uint32)
        n_nodes = int(faces.max()) + 1 if faces.size > 0 else 0
        cost_time = time.time() - start_time
        print(f"âœ… STLè¯»å–å®Œæˆï¼ˆåŒç›®å½•ï¼‰")
        print(f"   è€—æ—¶ï¼š{cost_time:.2f}s | é¡¶ç‚¹æ•°ï¼š{n_nodes:,} | ä¸‰è§’å½¢æ•°ï¼š{len(faces):,}")
        return faces, n_nodes
    except Exception as e:
        raise RuntimeError(f"âŒ STLè¯»å–å¤±è´¥ï¼š{str(e)}") from e

def generate_edges(faces):
    """å‘é‡åŒ–ç”Ÿæˆæ‰€æœ‰è¾¹ï¼ˆå•çº¿ç¨‹ï¼Œæ— å¹¶è¡Œå¼€é”€ï¼‰"""
    start_time = time.time()
    if faces.size == 0:
        return np.array([], dtype=np.uint32).reshape(0, 2)
    all_edges = np.concatenate([
        faces[:, [0, 1]],
        faces[:, [1, 2]],
        faces[:, [2, 0]]
    ], axis=0)
    all_edges_sorted = np.sort(all_edges, axis=1)
    cost_time = time.time() - start_time
    print(f"âœ… è¾¹ç”Ÿæˆå®Œæˆ")
    print(f"   è€—æ—¶ï¼š{cost_time:.2f}s | æ€»è¾¹æ•°ï¼š{len(all_edges_sorted):,}")
    return all_edges_sorted

def parallel_deduplicate_edges(edges, n_jobs=-1):
    """å¤šæ ¸å¹¶è¡Œå»é‡è¾¹ï¼ˆæ ¸å¿ƒåŠ é€Ÿæ­¥éª¤ï¼‰"""
    start_time = time.time()
    n_total = len(edges)
    if n_total < 100000:
        unique_edges = np.unique(edges, axis=0)
        cost_time = time.time() - start_time
        print(f"âœ… è¾¹å»é‡å®Œæˆï¼ˆå•çº¿ç¨‹ï¼‰")
        print(f"   è€—æ—¶ï¼š{cost_time:.2f}s | å»é‡åè¾¹æ•°ï¼š{len(unique_edges):,}")
        return unique_edges
    
    n_cores = os.cpu_count() if n_jobs == -1 else min(n_jobs, os.cpu_count())
    print(f"âœ… å¯åŠ¨å¤šæ ¸å»é‡ï¼ˆ{n_cores}ä¸ªæ ¸å¿ƒï¼‰")
    
    chunk_size = n_total // n_cores
    edge_chunks = [edges[i*chunk_size : (i+1)*chunk_size] for i in range(n_cores)]
    if n_total % n_cores != 0:
        edge_chunks[-1] = np.concatenate([edge_chunks[-1], edges[n_cores*chunk_size:]], axis=0)
    
    def deduplicate_chunk(chunk):
        return np.unique(chunk, axis=0)
    
    parallel_results = Parallel(n_jobs=n_jobs, backend='loky')(
        delayed(deduplicate_chunk)(chunk) for chunk in edge_chunks
    )
    
    # åˆå¹¶åˆ†ç‰‡å¹¶å…¨å±€å»é‡
    merged_edges = np.concatenate(parallel_results, axis=0)
    unique_edges = np.unique(merged_edges, axis=0)
    
    cost_time = time.time() - start_time
    print(f"âœ… è¾¹å»é‡å®Œæˆï¼ˆå¤šæ ¸ï¼‰")
    print(f"   è€—æ—¶ï¼š{cost_time:.2f}s | å»é‡åè¾¹æ•°ï¼š{len(unique_edges):,}")
    return unique_edges

def save_csv_to_same_dir(unique_edges, csv_file_name="sparse_adj_export.csv", save_symmetric=False):
    """å°†ç¨€ç–CSVä¿å­˜åˆ°è„šæœ¬åŒç›®å½•"""
    start_time = time.time()
    n_edges = len(unique_edges)
    script_dir = os.path.dirname(os.path.abspath(__file__))
    csv_path = os.path.join(script_dir, csv_file_name)
    
    # å¤„ç†ç©ºè¾¹æƒ…å†µ
    if n_edges == 0:
        with open(csv_path, 'w', encoding='utf-8') as f:
            f.write("row_index,col_index,value\n")
        print(f"âœ… ç©ºCSVå·²ä¿å­˜ï¼ˆåŒç›®å½•ï¼‰")
        print(f"   è·¯å¾„ï¼š{csv_path}")
        return
    
    # æ„å»ºCSVæ•°æ®ï¼ˆå‘é‡åŒ–ç”Ÿæˆï¼Œæé€Ÿï¼‰
    value_col = np.ones((n_edges, 1), dtype=np.uint8)
    csv_data = np.hstack([unique_edges, value_col])
    
    # æŒ‰éœ€ç”Ÿæˆå¯¹ç§°è¾¹ï¼ˆå¦‚(0,1)å’Œ(1,0)ï¼‰
    if save_symmetric:
        symmetric_edges = np.hstack([unique_edges[:, [1, 0]], value_col])
        csv_data = np.concatenate([csv_data, symmetric_edges], axis=0)
    
    # å¿«é€Ÿå†™å…¥CSVï¼ˆnp.savetxtæ¯”Pandaså¿«2~3å€ï¼‰
    with open(csv_path, 'w', encoding='utf-8') as f:
        f.write("row_index,col_index,value\n")
        np.savetxt(f, csv_data, fmt='%d', delimiter=',')
    
    # è®¡ç®—æ–‡ä»¶å¤§å°ï¼ˆç²¾ç¡®å€¼ï¼Œè€Œéä¼°ç®—ï¼‰
    file_size = os.path.getsize(csv_path) / 1024 / 1024  # è½¬ä¸ºMB
    cost_time = time.time() - start_time
    print(f"âœ… CSVä¿å­˜å®Œæˆï¼ˆåŒç›®å½•ï¼‰")
    print(f"   è€—æ—¶ï¼š{cost_time:.2f}s | è·¯å¾„ï¼š{csv_path} | å¤§å°ï¼š{file_size:.2f}MB")

def main():
    print("="*50)
    print("ğŸ“Œ STLè½¬ç¨€ç–CSVï¼ˆåŒç›®å½•ç‰ˆï¼Œå¤šæ ¸åŠ é€Ÿï¼‰")
    print("="*50)
    total_start = time.time()
    
    try:
        # 1. è¯»å–åŒç›®å½•export.stl
        faces, n_nodes = fast_read_stl()
        if n_nodes == 0:
            print("âŒ STLæ–‡ä»¶æ— é¡¶ç‚¹æ•°æ®ï¼Œé€€å‡º")
            return
        
        # 2. ç”Ÿæˆæ‰€æœ‰è¾¹
        all_edges = generate_edges(faces)
        
        # 3. å¤šæ ¸å¹¶è¡Œå»é‡è¾¹
        unique_edges = parallel_deduplicate_edges(all_edges, n_jobs=-1)
        
        # 4. ä¿å­˜CSVåˆ°åŒç›®å½•
        save_csv_to_same_dir(unique_edges, csv_file_name="sparse_adj_export.csv")
        
        # æ€»è€—æ—¶ç»Ÿè®¡
        total_cost = time.time() - total_start
        print("="*50)
        print(f"ğŸ‰ è½¬æ¢å®Œæˆï¼æ€»è€—æ—¶ï¼š{total_cost:.2f}s")
        print(f"   ğŸ‘‰ STLæºæ–‡ä»¶ï¼šåŒç›®å½•/export.stl")
        print(f"   ğŸ‘‰ CSVç»“æœæ–‡ä»¶ï¼šåŒç›®å½•/sparse_adj_export.csv")
        print("="*50)
    
    except Exception as e:
        print(f"\nâŒ è½¬æ¢å¤±è´¥ï¼š{str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    # å·²åˆ é™¤np.set_num_threads(1)ï¼Œé€‚é…æ—§ç‰ˆæœ¬NumPy
    main()
    